# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## üéØ Agent Forge V2: Documented Implementation

**CRITICAL CONTEXT**: This repository contains **complete V1 implementation documentation** including working systems for the first 4 phases. Agent Forge V2 builds upon proven V1 implementations with local-first architecture.

### Key Differences: V1 vs V2

| Aspect | V1 (Original - Documented) | V2 (This Build - In Progress) |
|--------|---------------------------|-------------------------------|
| **Purpose** | Server-based production system | Local research & development platform |
| **Deployment** | FastAPI, WebSocket, cloud infrastructure | Local CLI/notebook interface, consumer hardware |
| **Model Size** | 25M parameters (TRM √ó Titans-MAG), production-tested | **25M parameters** (Phase 1 Cognate), locally runnable |
| **Status** | Phases 1-4 fully implemented and documented | Building on V1 foundations, clean architecture |
| **Systems** | Modular Grokfast√óMuon optimizer, Prompt Baking, OpenRouter, W&B, UI | Reusing proven V1 systems, NASA POT10 compliance |
| **Our Use** | Reference implementation, proven systems | Implementation target with V1 learnings |

### What This Repository Contains

1. **V1 Implementation** (v1-reference/) - Complete production documentation for Phases 1-4
2. **Modular Systems** - Proven implementations:
   - **MuGrokfast** (Grokfast √ó Muon optimizer) - Production-ready, 50% training speedup
   - **Prompt Baking** - Full implementation for Phases 3, 5, 6
   - **OpenRouter Integration** - Frontier model access for data generation
   - **W&B Integration** - 100% complete across all 8 phases, 7,800+ metrics
   - **UI Implementation** - Full Next.js dashboard with WebSocket real-time updates
3. **Research Papers** (v1-reference/research-papers/) - Academic papers for all phases
4. **V2 Planning** (docs/v2-planning/) - Implementation plans and specifications
5. **Phase Documentation** (phases/phase1-8/) - Complete guides with LOGICAL_UNDERSTANDING

**‚úÖ PROVEN**: Phases 1-4 are production-ready with working implementations, test coverage >85%, and full documentation.

## Project Overview: Agent Forge V2

Agent Forge V2 is a **local-first 8-phase AI agent creation pipeline** that creates small, efficient models from scratch.

**Why V2?** V1 proved the 8-phase methodology works, but became unmaintainable. V2 rebuilds with:
- ‚úÖ **Local-first architecture**: Runs on consumer GPUs (GTX 1660+, 6GB+ VRAM)
- ‚úÖ **Small models**: Phase 1 creates 25M parameter models (Phase 1 TRM √ó Titans-MAG)
- ‚úÖ **Clean codebase**: NASA POT10 compliance from start, no technical debt
- ‚úÖ **Local W&B**: Weights & Biases integration for experiment tracking
- ‚úÖ **Proven methodology**: Keep the 8-phase pipeline that worked in V1

## Repository Structure

```
the agent maker/
‚îú‚îÄ‚îÄ phases/                    # Phase Documentation (V1 + V2)
‚îÇ   ‚îú‚îÄ‚îÄ phase1/               # Cognate - TRM √ó Titans-MAG (25M params) ‚úÖ IMPLEMENTED
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LOGICAL_UNDERSTANDING.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PHASE1_COMPLETE_GUIDE.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TRM_TITANS_ARCHITECTURE.md
‚îÇ   ‚îú‚îÄ‚îÄ phase2/               # EvoMerge - Evolutionary Optimization ‚úÖ IMPLEMENTED
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LOGICAL_UNDERSTANDING.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PHASE2_COMPLETE_GUIDE.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MERGE_TECHNIQUES_UPDATED.md (6 techniques)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ BINARY_PAIRING_STRATEGY.md
‚îÇ   ‚îú‚îÄ‚îÄ phase3/               # Quiet-STaR - Reasoning Enhancement ‚úÖ IMPLEMENTED
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LOGICAL_UNDERSTANDING.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PHASE3_COMPLETE_GUIDE.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PHASE3-QUIET-STAR-VISUALIZATION.md
‚îÇ   ‚îú‚îÄ‚îÄ phase4/               # BitNet - 1.58-bit Compression ‚úÖ IMPLEMENTED
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LOGICAL_UNDERSTANDING.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PHASE4_COMPLETE_GUIDE.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ production-readiness-assessment.md
‚îÇ   ‚îú‚îÄ‚îÄ phase5-8/             # Phases 5-8 documented, implementation in progress
‚îÇ
‚îú‚îÄ‚îÄ docs/                      # V2 Documentation (REORGANIZED)
‚îÇ   ‚îú‚îÄ‚îÄ INDEX.md              # Master documentation index
‚îÇ   ‚îú‚îÄ‚îÄ v2-specification/     # V2 technical specifications
‚îÇ   ‚îú‚îÄ‚îÄ v2-planning/          # Implementation plans (11 files)
‚îÇ   ‚îú‚îÄ‚îÄ integration/          # Integration guides (UI, W&B)
‚îÇ   ‚îú‚îÄ‚îÄ datasets/             # Dataset documentation
‚îÇ   ‚îî‚îÄ‚îÄ graphviz/             # Process flow diagrams
‚îÇ
‚îú‚îÄ‚îÄ v1-reference/             # V1 Implementation Documentation
‚îÇ   ‚îú‚îÄ‚îÄ analysis/             # V1 analysis and findings
‚îÇ   ‚îú‚îÄ‚îÄ planning/             # V1 planning iterations
‚îÇ   ‚îú‚îÄ‚îÄ implementation/       # V1 code documentation (23 files)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MUGROKFAST_DEVELOPER_GUIDE.md (900+ lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PROMPT_BAKING_INTEGRATION.md (1,339 lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ WANDB_100_PERCENT_COMPLETE.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ COMPLETE_IMPLEMENTATION_SUMMARY.md
‚îÇ   ‚îú‚îÄ‚îÄ architecture/         # V1 system architecture
‚îÇ   ‚îî‚îÄ‚îÄ research-papers/      # Academic papers (PDFs)
‚îÇ
‚îú‚îÄ‚îÄ README.md                 # Project overview
‚îú‚îÄ‚îÄ CLAUDE.md                 # This file (AI assistant instructions)
‚îî‚îÄ‚îÄ FILE_MANIFEST.txt         # Complete file index
```

## 8-Phase Pipeline Architecture

Agent Forge V2 preserves the proven 8-phase methodology from V1, but with clean implementation:

1. **Phase 1 (Cognate)**: Create 3x **25M parameter** TRM √ó Titans-MAG models ‚úÖ **IMPLEMENTED**
   - **Key Innovation**: Small enough to run locally (GTX 1660, 6GB VRAM)
   - ACT (Adaptive Computation Time) + LTM (Long-Term Memory)
   - **V1 Implementation**: TRM architecture with 3 specialized models (reasoning, memory, general)
   - **Systems**: MuGrokfast optimizer (10% faster convergence), W&B tracking (37 metrics)
   - **Model Management**: Handoff validation, auto-cleanup, 99% reconstruction success
   - Research: TRM √ó Titans-MAG paper, HRM (Hierarchical Reasoning Models)

2. **Phase 2 (EvoMerge)**: 50 generations evolutionary optimization ‚úÖ **IMPLEMENTED**
   - **6 merge techniques**: Linear, SLERP, TIES, DARE, FrankenMerge, DFS
   - **Binary pairing strategy**: Efficient population management
   - **V1 Implementation**: 370 W&B metrics, handoff validation from Phase 1
   - **Results**: 23.5% fitness gain, 90-min GPU time, champion model selection
   - **Model Storage**: Metadata versioning v2.0, automatic cleanup

3. **Phase 3 (Quiet-STaR)**: Reasoning enhancement via thought generation ‚úÖ **IMPLEMENTED**
   - **Token-wise parallel thought sampling**: Generate internal reasoning
   - **Coherence scoring**: Semantic, syntactic, predictive metrics
   - **Prompt Baking Integration**: Bake CoT reasoning before RL training (5 min)
   - **OpenRouter Integration**: Frontier model data generation ($100-200 cost)
   - **Anti-Theater Detection**: Validate genuine reasoning vs memorization
   - **V1 Implementation**: >85% test coverage, 17 W&B metrics, two-step workflow

4. **Phase 4 (BitNet)**: 1.58-bit quantization ‚úÖ **IMPLEMENTED**
   - **Target**: 8.2x compression, 3.8x speedup
   - **STE (Straight-Through Estimator)**: Quantized forward, full-precision gradients
   - **MuGrokfast Compatibility**: STE mode enabled for gradient flow
   - **V1 Implementation**: Production-ready, CI/CD integration validated
   - **Results**: 19 W&B metrics, validated compression ratios
   - Research: BitNet 1.58-bit paper, Fine-tuning to 1.58bit

5. **Phase 5 (Curriculum Learning)**: 7-stage adaptive curriculum with frontier models ‚úÖ **V2 REDESIGN**
   - **Edge-of-Chaos Assessment**: Find optimal difficulty (75% accuracy threshold)
   - **Adaptive Curriculum**: 20,000 questions across 10 levels
   - **Tool Use Training**: Code execution with validation
   - **Eudaimonia Baking**: 4-rule moral system (see PHASE5_EUDAIMONIA_SYSTEM.md - 589 lines complete)
   - **Self-Modeling**: Temperature range prediction training
   - **Dream Consolidation**: 3 epochs per level √ó 10 levels = 5-10 hours total
     - Full autoencoder (encoder + decoder) reconstruction
     - High-temperature replay (T=1.2) for creative problem-solving
     - Prevents catastrophic forgetting across curriculum levels
     - Based on "Dreaming is All You Need" paper (phases/phase5/)
   - **Frontier Models**: GPT-4o-mini, Claude-3.5 Haiku, Gemini 2.0 Flash, Qwen 2.5
   - **Cost**: $600-800 OpenRouter API, 120-240 hours training
   - **Duration**: 12-24 hours per level √ó 10 levels
   - **V2 Documentation**: PHASE5_LOGICAL_UNDERSTANDING_V2.md, PHASE5_EUDAIMONIA_SYSTEM.md, PHASE5_V2_IMPLEMENTATION_SUMMARY.md
   - Research: Intelligence at Edge of Chaos, Self-Modeling, Dreaming is All You Need papers

6. **Phase 6 (Tool & Persona Baking)**: Iterative A/B optimization loops ‚úÖ **V2 REDESIGN**
   - **A-Cycle**: Tool use optimization via SWE-Bench
   - **B-Cycle**: Self-guided persona generation (model discovers own patterns)
   - **NOT 9 pre-defined personas** (that was V1) - Model-driven evolution
   - **Half-Baking Strategy**: 50% strength per iteration
   - **Plateau Detection**: Automatic cycle switching
   - **V2 Documentation**: PHASE6_COMPLETE_GUIDE.md (70.1% ‚Üí 95% target), LOGICAL_UNDERSTANDING.md
   - Research: Prompt Baking paper (arXiv:2409.13697v1)

7. **Phase 7 (Self-Guided Experts)**: Model-driven expert discovery & architecture search ‚úÖ **V2 REDESIGN**
   - **Stage 1**: Model analyzes own capabilities, determines expert count (N=3-10)
   - **Stage 2**: Transformer¬≤ SVF training (REINFORCE + MuonGrokfast fallback)
   - **Stage 3**: Model-guided NSGA-II ADAS (100 gen √ó 50 pop = 5000 evaluations)
   - **NOT manual ADAS** (that was V1) - Self-guided system
   - **Frontier Models**: Same as Phase 5 (GPT-4o-mini, Claude-3.5 Haiku, Gemini 2.0 Flash, Qwen 2.5)
   - **Cost**: $150-250 OpenRouter, 42 hours ADAS + 36 hours SVF = 78 hours total
   - **V2 Documentation**: PHASE7_COMPLETE_GUIDE.md, PHASE7_SELF_GUIDED_SYSTEM.md, LOGICAL_UNDERSTANDING.md
   - Research: Transformer¬≤ SVF paper, NSGA-II, Automated Design of Agentic Systems

8. **Phase 8 (Final Compression)**: ‚úÖ **PRODUCTION READY** with benchmark testing
   - **Three-Stage Pipeline**: SeedLM (2√ó) ‚Üí VPTQ (20√ó) ‚Üí Hypercompression (6.25√ó)
   - **Target**: 280√ó compression (100MB ‚Üí 0.4MB)
   - **Benchmark Testing**: 7 core benchmarks + expert-specific + Phase 5 integration tests
     - ‚â•95% retention (SeedLM/VPTQ stages)
     - ‚â•84% final retention (cumulative after all 3 stages)
   - **Quality Gates**: Automatic rollback to VPTQ (2.5MB) or SeedLM (50MB) if quality fails
   - **Validation Time**: 27 hours baseline + compression, 40-50 hours with retries
   - **User Requirement Met**: "uses benchmark testing to make sure we dont lose to much quality"
   - **V2 Status**: Complete (1080 lines) - PHASE8_COMPLETE_GUIDE.md, PHASE8_BENCHMARK_TESTING.md
   - Research: SeedLM paper, VPTQ paper, Hyper-Compression paper

## Modular Systems (V1 Implementation - Production Ready)

### 1. MuGrokfast Optimizer (Grokfast √ó Muon)

**Location**: `v1-reference/implementation/MUGROKFAST_DEVELOPER_GUIDE.md` (913 lines)

**What It Is**: Unified optimizer combining three complementary techniques:
- **Grokfast (Time-Spectrum)**: EMA gradient filtering, accelerates "grokking" phenomenon
- **Muon (Space-Geometry)**: Newton-Schulz orthogonalization, prevents low-rank collapse
- **QK-Clip/MuonClip**: Attention safety rails for RL training

**Implementation**:
```python
from mugrokfast_optimizer import MuonGrokfast
from mugrokfast_config import MuGrokConfig

# Phase-specific presets
config = MuGrokConfig.from_phase(1)  # Auto-configured for Phase 1
optimizer = MuonGrokfast(model.parameters(), config=config)
```

**Key Features**:
- **Parameter Routing**: 2-D params ‚Üí Muon, 1-D params ‚Üí fallback (AdamW/Lion)
- **Phase Presets**: Optimized configs for Phases 1, 3, 5, 6, 7
- **STE Compatibility**: Phase 5 BitNet integration (muon_ste_mode=True)
- **Proven Results**: 10-50% training speedup vs baseline

**Phase-Specific Configs**:
- Phase 1: muon_lr=0.01, grokfast_lambda=0.05 (gentle filtering)
- Phase 3: muon_lr=5e-4, qk_clip_threshold=25.0 (RL stability)
- Phase 5: muon_ste_mode=True, grokfast_lambda=2.0 (aggressive)

---

### 2. Prompt Baking System

**Location**: `v1-reference/implementation/PROMPT_BAKING_INTEGRATION.md` (1,339 lines)

**What It Is**: Converts prompts into weight updates, "baking" behavior into model without needing prompts at inference.

**Core Algorithm**:
```
Œ∏_u = argmin D_KL(P_Œ∏(¬∑|u) || P_Œ∏u(¬∑))
```
Where Œ∏_u are baked weights that behave like prompted model P_Œ∏(¬∑|u)

**Features**:
- **Fast**: 5 minutes per prompt (LoRA-based)
- **Half-Baking**: Stop early for partial prompt strength
- **Prompt Pursuit**: Iterative re-baking for amplification (15-40% gains)
- **Sequential Baking**: Compose multiple prompts (Œ∏_u1u2 = B(B(Œ∏, u1), u2))
- **No Prompt Decay**: Maintains behavior over 30+ turns

**Usage Across Phases**:
| Phase | Type | Prompts | Time | Purpose |
|-------|------|---------|------|---------|
| Phase 3 | Reasoning | 1 CoT | 5 min | Stabilize RL training |
| Phase 5 | Training Efficiency | 1 | 5 min | Accelerate convergence |
| Phase 6 | **HEAVY** Persona | 10 (9 agents + tools) | 50 min | Create specialized agents |

**Implementation**:
```python
from prompt_baking import bake_prompt, PromptBakingConfig

config = PromptBakingConfig(lora_r=16, num_epochs=3)
baked_model = bake_prompt(model, "You are a reasoning specialist...", config)
```

---

### 3. OpenRouter Integration (Frontier Models)

**Location**: `v1-reference/implementation/` (openrouter_client.py, frontier_model_generator.py)

**What It Is**: Modular system for accessing frontier models (GPT-4, Claude, etc.) for data generation.

**Primary Use**: Phase 3 Quiet-STaR data generation
- Generate reasoning trajectories from frontier models
- Cost: $100-200 for full Phase 3 dataset
- Supports multiple providers via OpenRouter API

**Features**:
- Model selection (GPT-4, Claude-3, etc.)
- Cost tracking and budget limits
- Batch generation with retry logic
- Quality filtering

---

### 4. Weights & Biases Integration (100% Complete)

**Location**: `v1-reference/implementation/WANDB_100_PERCENT_COMPLETE.md`

**Status**: ‚úÖ **ALL 8 PHASES INTEGRATED** (7,800+ total metrics)

**Per-Phase Metrics**:
- Phase 1 (Cognate): 37 metrics - 3 model training
- Phase 2 (EvoMerge): 370 metrics - 50 generation evolution
- Phase 3 (Quiet-STaR): 17 metrics - Reasoning + coherence
- Phase 4 (BitNet): 19 metrics - 8√ó compression validation
- Phase 5 (Forge): 7,208 metrics - 50K training steps
- Phase 6 (Baking): 25 metrics - Tool/persona patterns
- Phase 7 (ADAS): 100 metrics - Architecture discovery
- Phase 8 (Final): 25 metrics - 280√ó compression

**Features**:
- Real-time experiment tracking
- Model artifact management
- Cross-phase continuity tables
- Automatic metric logging
- Dashboard configuration

**Usage**:
```python
import wandb

wandb.init(project="agent-forge-v2", config=config.to_dict())
wandb.log({'loss': loss.item(), 'epoch': epoch})
```

---

### 5. UI Implementation (Next.js Dashboard)

**Location**: `docs/integration/` (UI_INTEGRATION_README.md, PHASES_UI_INTEGRATION_GUIDE.md)

**Status**: Complete specifications for Phases 1-4 (150+ pages)

**Features**:
- **Real-time WebSocket updates**: Live training progress
- **Phase Handoff Component**: Validation, model metadata, cleanup controls
- **Storage Management Page**: Track models, auto-cleanup, stats
- **3D Merge Visualization**: Three.js visualization of EvoMerge tree
- **Anti-Theater Detection**: Phase 3 reasoning validation display
- **Mobile Responsive**: Full mobile/tablet support
- **Accessibility**: WCAG 2.1 AA compliant

**Tech Stack**:
- Next.js 14 (App Router)
- React 18
- WebSocket (real-time)
- Three.js (3D visualization)
- Tailwind CSS
- TypeScript

**API Endpoints Per Phase**: 7-8 REST + WebSocket endpoints

---

## V2 Technology Stack (Local-First)

### Core ML/AI
- **Python 3.10+** - Primary language
- **PyTorch 2.0+** - Model framework
- **HuggingFace Transformers** - Model utilities
- **Weights & Biases (local)** - Experiment tracking

### Local Development
- **Jupyter Notebooks** - Interactive development (no FastAPI server)
- **CLI Tools** - Command-line interface for pipeline execution
- **Local GPU** - CUDA-capable GPU (GTX 1660 or better, 6GB+ VRAM)
- **16GB+ RAM** - System memory requirement
- **50GB disk** - Storage for models and checkpoints

### Not Used in V2 (V1 Only)
- ~~FastAPI~~ (V1 server framework)
- ~~WebSocket~~ (V1 real-time updates)
- ~~Next.js~~ (V1 frontend)
- ~~SQLite~~ (V1 model registry)
- ~~S3~~ (V1 cloud storage)

## Key Documentation Files

### Master Index
- **[docs/INDEX.md](docs/INDEX.md)** - Complete documentation navigation with role-based guides

### V1 Implementation Documentation (Production-Ready)
**Location**: `v1-reference/implementation/` (23 files)

**Core Systems** (Complete Implementation Guides):
1. **MUGROKFAST_DEVELOPER_GUIDE.md** (913 lines) - Complete optimizer documentation
   - API reference, phase-specific usage, testing, W&B integration
2. **PROMPT_BAKING_INTEGRATION.md** (1,339 lines) - Full prompt baking system
   - Modular architecture, phase integrations, complete examples
3. **WANDB_100_PERCENT_COMPLETE.md** - All 8 phases integrated (7,800+ metrics)
4. **COMPLETE_IMPLEMENTATION_SUMMARY.md** - Phase handoff implementation
   - 99% model reconstruction, auto-cleanup, validation system

**Architecture Documentation**:
- **v1-reference/architecture/** - API, backend, frontend specifications
- **v1-reference/research-papers/** - Academic papers (Grokfast, Muon, Prompt Baking, etc.)

### V2 Planning & Specifications
**Location**: `docs/v2-planning/` (11 files)

1. **PHASE1-4_COMPREHENSIVE_PLAN_V2.md** - Technical implementation plans
2. **PHASE1-4_IMPLEMENTATION_CHECKLIST.md** - Step-by-step checklists
3. **PHASE1-4_PREMORTEM_V2.md** - Risk analysis for V2 build
4. **PHASE1-4_MASTER_INDEX.md** - Navigation hub for Phases 1-4
5. **PHASE1-3_CONSISTENCY_ANALYSIS.md** - Cross-phase compatibility

### Integration Guides
**Location**: `docs/integration/` (5 files)

1. **UI_INTEGRATION_README.md** - Complete UI documentation (150+ pages)
2. **PHASES_UI_INTEGRATION_GUIDE.md** - Quick reference per phase
3. **WANDB_INTEGRATION_GUIDE.md** - W&B setup and usage
4. **PHASE1-4_WANDB_INTEGRATION.md** - Phase-specific W&B integration
5. **UI_INTEGRATION_COMPLETE.md** - Implementation status

### Dataset Documentation
**Location**: `docs/datasets/` (5 files)

1. **PHASE1_DATASET_SPECIFICATION.md** - Phase 1 training data
2. **PHASE3_DATA_GENERATION_GUIDE.md** - OpenRouter data generation
3. **PHASE3_PROMPT_BAKING_CORRECTED.md** - Prompt baking for Phase 3

### Phase-Specific Documentation
**Location**: `phases/phase1/` through `phases/phase8/`

Each phase contains:
- **LOGICAL_UNDERSTANDING.md** - Research synthesis + implementation approach
- **PHASEN_COMPLETE_GUIDE.md** - Complete implementation guide (V1)
- **README.md** - Phase overview
- **graphviz/** - Process flow diagrams (where available)
- Phase-specific technical documents

**Phase 1 Example**:
- TRM_TITANS_ARCHITECTURE.md - TRM √ó Titans-MAG model architecture
- PREMORTEM_CHECKLIST.md - Risk assessment

**Phase 2 Example**:
- MERGE_TECHNIQUES_UPDATED.md - 6 merge techniques documentation
- BINARY_PAIRING_STRATEGY.md - Evolution strategy

### Visual Documentation
**Location**: `docs/graphviz/` (3 files)

- agent-forge-master-flow.dot - Master workflow diagram
- phase-integration-flow.dot - Phase handoff flows
- GRAPHVIZ_UPDATE_SUMMARY.md - GraphViz documentation updates

### V1 Analysis (Historical Context)
**Location**: `v1-reference/analysis/` (6 files)

- **LOOP1-COMPLETE-SUMMARY.md** - Complete V1 analysis (93% confidence)
- **FINDINGS-AGGREGATION.md** - Research findings aggregation
- **architecture-analysis.md** - V1 architecture review
- **code-quality-report.md** - V1 quality assessment

## Development Workflow for V2

### Starting V2 Implementation
1. **Read this file** (CLAUDE.md) - Understand V1 vs V2 distinction
2. **Read V2-IMPLEMENTATION-GUIDE.md** - Step-by-step build instructions
3. **Review phase LOGICAL_UNDERSTANDING.md docs** - Understand each phase conceptually
4. **Check GraphViz flows** - Visual process documentation
5. **Start with Phase 1** - Build Cognate (3x 25M param models)

### Do NOT Do (V1-Specific Work)
- ‚ùå Refactor God objects (V1 code, not in V2)
- ‚ùå Delete 201 backup files (V1 code, not in V2)
- ‚ùå Fix Phase 5 syntax errors (V1 code, building V2 clean)
- ‚ùå Redesign Phase 7 ADAS (V2 is generic edge deployment from start)
- ‚ùå Break existing Phases 2/3/4 (V1 code, not in V2)

### V2 Build Order (16-Week Timeline)
- **Weeks 1-2**: Environment setup + Phase 1 (Cognate)
  - Python environment, PyTorch, local W&B
  - Implement TRM √ó Titans-MAG architecture (25M params)
  - Train 3 specialized models
  - Validate models run on local GPU

- **Weeks 3-4**: Phase 2 (EvoMerge)
  - Implement 6 merge techniques
  - Run 50 generations (or optimize for local speed)
  - Fitness evaluation and selection

- **Weeks 5-6**: Phase 3 (Quiet-STaR)
  - Implement thought generation
  - Coherence scoring
  - Validate reasoning (anti-theater checks)

- **Weeks 7-8**: Phase 4 (BitNet)
  - 1.58-bit quantization
  - Validate 8.2x compression
  - Local inference testing

- **Weeks 9-10**: Phase 5 (Forge Training)
  - Combined BitNet + Grokfast
  - Validate 50% speedup claim
  - Local GPU optimization

- **Weeks 11-12**: Phases 6-8
  - Phase 6: Prompt baking (9 agents)
  - Phase 7: Generic edge deployment
  - Phase 8: Final compression

- **Weeks 13-16**: Integration + Testing
  - End-to-end pipeline
  - Local W&B dashboard
  - Performance benchmarking
  - Documentation completion

## Quality Standards (V2)

### Code Quality (From Day 1)
- ‚úÖ **NASA POT10**: All functions ‚â§60 LOC (enforced via pre-commit hook)
- ‚úÖ **Test Coverage**: ‚â•90% overall, ‚â•95% for critical paths
- ‚úÖ **Type Hints**: ‚â•98% coverage
- ‚úÖ **Documentation**: ‚â•95% function docstrings
- ‚úÖ **No God Objects**: Max 500 LOC per file
- ‚úÖ **Version Control**: Git branches only (no backup files)

### Performance (Local Hardware)
- ‚úÖ **Phase 1 Models**: 25M params, fit in 6GB VRAM
- ‚úÖ **Training Time**: Reasonable on consumer GPU (hours, not days)
- ‚úÖ **Inference**: Real-time on GTX 1660 or better
- ‚úÖ **Memory**: ‚â§16GB system RAM

## File Organization

**NEVER save files to the root directory**. Use these subdirectories:
- `docs/` - V2 documentation and markdown files
- `phases/phaseN/` - Phase-specific documentation (V1 + V2)
- `src/` - V2 source code (when implementation begins)
- `tests/` - V2 test files
- `examples/` - V2 example notebooks/scripts

## Common Tasks

### Understanding V2 Project
```bash
# Read V2 overview
Read CLAUDE.md  # This file

# Read V2 implementation guide
Read docs/V2-IMPLEMENTATION-GUIDE.md

# Check V1 vs V2 comparison
Read docs/V1-vs-V2-COMPARISON.md

# Review phase understanding
Read phases/phase1/LOGICAL_UNDERSTANDING.md
```

### Understanding V1 (Reference Only)
```bash
# V1 analysis summary
Read LOOP1-COMPLETE-SUMMARY.md

# V1 refactoring plan (NOT V2 build plan)
Read PLAN-v3.md

# V1 phase implementations (reference)
Read phases/phase2/PHASE2_COMPLETE_GUIDE.md
```

### Starting V2 Implementation
```bash
# Setup environment
# Follow V2-IMPLEMENTATION-GUIDE.md

# Implement Phase 1
# Build TRM √ó Titans-MAG models (25M params each)

# Validate locally
# Run on local GPU, verify fits in 6GB VRAM
```

## Critical Insights for V2

### What We're Keeping from V1 (Proven Systems)

**‚úÖ Production-Ready Systems to Reuse:**

1. **MuGrokfast Optimizer** - Complete, documented, proven
   - 913-line developer guide with full API reference
   - Phase-specific presets for Phases 1, 3, 5, 6, 7
   - 10-50% training speedup validated
   - Ready for direct integration

2. **Prompt Baking System** - Complete modular implementation
   - 1,339-line integration guide with examples
   - Half-baking, prompt pursuit, sequential baking all implemented
   - Used across Phases 3, 5, 6 (especially heavy in Phase 6)
   - 5-minute baking time validated

3. **W&B Integration** - 100% complete across all 8 phases
   - 7,800+ metrics defined and documented
   - Cross-phase continuity tracking
   - Dashboard configurations ready
   - Just needs local W&B setup

4. **Phase 1-4 Implementations** - Production-tested
   - Phase handoff validation (99% success rate)
   - Model storage with auto-cleanup
   - 6 merge techniques for Phase 2 (23.5% fitness gain)
   - STE implementation for Phase 4

5. **UI Architecture** - Complete specifications
   - 150+ pages of UI/UX design
   - 7-8 API endpoints per phase
   - WebSocket real-time updates
   - Can be adapted for V2 local interface

6. **OpenRouter Integration** - Modular frontier model access
   - Phase 3 data generation ($100-200 cost)
   - Quality filtering and cost tracking
   - Ready for local use

### What We're Adapting from V1

1. **Architecture ‚Üí Local-First**
   - V1: FastAPI server + Next.js UI ‚Üí V2: Local CLI/notebook
   - V1: Cloud storage (S3) ‚Üí V2: Local filesystem
   - V1: Remote GPU ‚Üí V2: Consumer GPU (6GB+ VRAM)
   - **Keep**: All algorithms, just change deployment

2. **Phase 7 ‚Üí Generic Edge**
   - V1: ADAS (automotive-specific) ‚Üí V2: Generic edge deployment
   - **Keep**: Architecture discovery approach, just broader scope

3. **Model Size ‚Üí Validated**
   - V1: Documented 25M params ‚Üí V2: **Same**, now validated as local-runnable
   - **Keep**: Exact TRM √ó Titans-MAG architecture that V1 proved works

### V2-Specific Improvements

1. ‚úÖ **NASA POT10 Compliance** - From day 1, not retrofitted
2. ‚úÖ **No Technical Debt** - Clean build using V1's working systems
3. ‚úÖ **Local W&B** - Self-hosted experiment tracking
4. ‚úÖ **Simplified Deployment** - No server setup required
5. ‚úÖ **V1 Lessons Learned** - Build on successes, avoid pitfalls

### Key Advantage

**V2 is NOT starting from scratch.** We have:
- 5 complete modular systems ready to integrate
- 4 phases fully documented with working implementations
- 7,800+ metrics already defined
- All research papers and techniques validated
- **Estimated V2 development time: 8-12 weeks** (vs 16+ from zero)

## Success Metrics for V2

### Technical Success
- ‚úÖ All 8 phases implemented and working
- ‚úÖ Phase 1 creates 25M param models that run locally
- ‚úÖ End-to-end pipeline completes on local machine
- ‚úÖ 100% NASA POT10 compliance
- ‚úÖ ‚â•90% test coverage
- ‚úÖ Models perform well on local GPU (inference <100ms)

### Research Success
- ‚úÖ Validate Grokfast 50x speedup claim (or document actual speedup)
- ‚úÖ Confirm Phase 1-8 integration works as designed
- ‚úÖ Document any deviations from research papers
- ‚úÖ Create reproducible local setup

## For Different Roles

### V2 Developers (Implementing V2)
1. Read: This file (CLAUDE.md) - Understand V1 vs V2
2. Read: docs/V2-IMPLEMENTATION-GUIDE.md - Build instructions
3. Review: phases/phaseN/LOGICAL_UNDERSTANDING.md - Phase concepts
4. Reference: V1 PHASEN_COMPLETE_GUIDE.md - Implementation ideas (but build clean)

### V1 Researchers (Understanding Original)
1. Read: LOOP1-COMPLETE-SUMMARY.md - V1 analysis
2. Review: PLAN-v3.md - V1 refactoring plan
3. Study: phases/phaseN/PHASEN_COMPLETE_GUIDE.md - V1 implementations

### Project Managers
1. Read: docs/V1-vs-V2-COMPARISON.md - Understand project pivot
2. Review: PLAN-V2-BUILD.md - V2 timeline (16 weeks vs V1's 26 weeks)
3. Check: PREMORTEM-V2-BUILD.md - V2 risks (different from V1 risks)

## Important Reminders

- This is a **comprehensive implementation documentation repository**
- **V1 Phases 1-4**: Fully implemented, production-tested, documented
- **V2 Build**: Reuse V1's proven modular systems, adapt to local-first
- Focus on **local-first** - no cloud infrastructure, consumer hardware only
- Keep models **small** - 25M params validated as local-runnable
- Build **clean** - NASA POT10 from day 1, using V1's working systems
- Never create files in root directory
- Always use subdirectories: `docs/`, `phases/phaseN/`, `v1-reference/`, `src/`, `tests/`

## Quick Start for New Contributors

### 1. Understanding the Project
```bash
# Read project overview
cat README.md

# Read this file for complete context
cat CLAUDE.md

# Browse master documentation index
cat docs/INDEX.md
```

### 2. Explore V1 Implementations (Production-Ready)
```bash
# MuGrokfast optimizer (ready to use)
cat v1-reference/implementation/MUGROKFAST_DEVELOPER_GUIDE.md

# Prompt baking system (ready to use)
cat v1-reference/implementation/PROMPT_BAKING_INTEGRATION.md

# W&B integration (100% complete)
cat v1-reference/implementation/WANDB_100_PERCENT_COMPLETE.md

# Phase handoff system
cat v1-reference/implementation/COMPLETE_IMPLEMENTATION_SUMMARY.md
```

### 3. Review Phase Documentation
```bash
# Phase 1 (Cognate) - Complete implementation
cat phases/phase1/LOGICAL_UNDERSTANDING.md
cat phases/phase1/PHASE1_COMPLETE_GUIDE.md

# Phase 2 (EvoMerge) - 6 merge techniques
cat phases/phase2/PHASE2_COMPLETE_GUIDE.md
cat phases/phase2/MERGE_TECHNIQUES_UPDATED.md

# Phase 3 (Quiet-STaR) - Reasoning + prompt baking
cat phases/phase3/PHASE3_COMPLETE_GUIDE.md
cat docs/datasets/PHASE3_DATA_GENERATION_GUIDE.md

# Phase 4 (BitNet) - 1.58-bit quantization
cat phases/phase4/PHASE4_COMPLETE_GUIDE.md
```

### 4. Check V2 Planning
```bash
# V2 implementation plans
cat docs/v2-planning/PHASE1-4_COMPREHENSIVE_PLAN_V2.md
cat docs/v2-planning/PHASE1-4_IMPLEMENTATION_CHECKLIST.md
cat docs/v2-planning/PHASE1-4_PREMORTEM_V2.md
```

### 5. Integration Guides
```bash
# UI specifications
cat docs/integration/UI_INTEGRATION_README.md

# W&B setup
cat docs/integration/WANDB_INTEGRATION_GUIDE.md
```

## Repository Statistics

### Documentation
- **Total Files**: ~150+ documentation files
- **V1 Implementation Docs**: 23 files (v1-reference/implementation/)
- **Phase Guides**: 8 phases √ó 3-5 files each = ~35 files
- **V2 Planning**: 11 files (docs/v2-planning/)
- **Integration Guides**: 5 files (docs/integration/)
- **Total Documentation**: 50,000+ words

### Code Documentation
- **MuGrokfast**: 913 lines (complete API, examples, tests)
- **Prompt Baking**: 1,339 lines (full system, all features)
- **Phase Implementations**: Phases 1-4 fully documented
- **W&B Integration**: 7,800+ metrics across 8 phases

### Implementation Status
- ‚úÖ **Phase 1 (Cognate)**: Complete - 3 TRM √ó Titans-MAG models, TRM architecture
- ‚úÖ **Phase 2 (EvoMerge)**: Complete - 6 merge techniques, 23.5% gain
- ‚úÖ **Phase 3 (Quiet-STaR)**: Complete - Reasoning, prompt baking, anti-theater
- ‚úÖ **Phase 4 (BitNet)**: Complete - 1.58-bit quantization, STE
- üìã **Phase 5 (Forge)**: Documented - MuGrokfast system ready
- üìã **Phase 6 (Baking)**: Documented - 9 agents, prompt baking system ready
- üìã **Phase 7 (ADAS)**: Planned - Architecture discovery
- üìã **Phase 8 (Final)**: Planned - Triple compression

### Modular Systems (Ready to Integrate)
1. ‚úÖ **MuGrokfast Optimizer** - Production-ready
2. ‚úÖ **Prompt Baking** - Production-ready
3. ‚úÖ **OpenRouter Integration** - Production-ready
4. ‚úÖ **W&B Integration** - 100% complete
5. ‚úÖ **UI Specifications** - Complete design

---

**Agent Forge V2**: Building on proven V1 foundations. Local-first. 25M parameter models. 5 modular systems ready. Phases 1-4 documented. 8-12 week build timeline.
