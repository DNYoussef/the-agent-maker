# Phase 5: Curriculum Management System

**Version**: 2.0
**Purpose**: Detailed specification of the adaptive curriculum learning system
**Focus**: Question lifecycle, dataset dynamics, variant generation, hint management

---

## Table of Contents

1. [System Overview](#system-overview)
2. [Question Lifecycle](#question-lifecycle)
3. [Dataset Structure](#dataset-structure)
4. [Variant Generation](#variant-generation)
5. [Hint Management](#hint-management)
6. [Dataset Shrinkage Mechanics](#dataset-shrinkage-mechanics)
7. [Implementation Specifications](#implementation-specifications)
8. [Metrics & Monitoring](#metrics--monitoring)

---

## System Overview

The Curriculum Management System is the "operating system" of Phase 5 training. It:

1. **Manages** ~20,000 initial questions across 10 levels
2. **Tracks** question state (active, variant, hinted, mastered)
3. **Generates** variants to prevent memorization
4. **Adds** hints for failed attempts
5. **Removes** mastered concepts
6. **Shrinks** dataset as proof of comprehension

**Key Innovation**: Dataset size is a **visible progress metric**
- Start: 2,000 questions/level
- End: 0-50 questions/level (95%+ mastery)

---

## Question Lifecycle

### State Machine

Every question progresses through states:

```
[INITIAL] → [ACTIVE] → [VARIANT] → [MASTERED]
                 ↓
            [HINTED] ──→ [VARIANT] → [MASTERED]
```

**State Definitions**:

1. **INITIAL**: Just generated by frontier model, never seen by student
2. **ACTIVE**: In rotation, presented to student model
3. **HINTED**: Failed validation, has 1+ hints attached
4. **VARIANT**: Succeeded, replaced by tweaked version (different nouns/numbers)
5. **MASTERED**: 3 consecutive successes, removed from dataset

---

### Detailed State Transitions

#### Transition 1: INITIAL → ACTIVE

**Trigger**: Question added to dataset

**Actions**:
```python
question = {
    "id": "q_1234",
    "text": "Implement binary search on a sorted array",
    "level": 3,
    "source": "gpt4",
    "state": "active",
    "hints": [],
    "attempt_count": 0,
    "success_count": 0,
    "consecutive_successes": 0,
    "variants": [],
    "created_at": timestamp
}
```

---

#### Transition 2A: ACTIVE → VARIANT (Success)

**Trigger**: Student model successfully validates question

**Process**:
1. **Increment counters**
   ```python
   question.success_count += 1
   question.consecutive_successes += 1
   ```

2. **Send to frontier model for variant generation**
   ```python
   variant_request = {
       "original_question": question.text,
       "instruction": "Create a variant by changing nouns and numbers, keeping core concept",
       "original_source": question.source
   }

   # Use DIFFERENT frontier model than original
   if question.source == "gpt4":
       variant_generator = "claude"
   else:
       variant_generator = "gpt4"

   variant_text = frontier_api.generate_variant(variant_request, model=variant_generator)
   ```

3. **Create new variant question**
   ```python
   variant_question = {
       "id": f"{question.id}_var_{question.success_count}",
       "text": variant_text,
       "level": question.level,
       "source": f"{variant_generator}_variant",
       "state": "active",
       "hints": [],  # Variants start with NO hints
       "attempt_count": 0,
       "success_count": 0,
       "consecutive_successes": 0,
       "parent_question": question.id,
       "created_at": timestamp
   }
   ```

4. **Replace original with variant**
   ```python
   dataset.remove(question.id)
   dataset.add(variant_question)

   # Store relationship
   question.variants.append(variant_question.id)
   question.state = "replaced_by_variant"
   archive.store(question)  # Keep for analysis
   ```

**Net dataset size change**: 0 (removed 1, added 1)

---

#### Transition 2B: ACTIVE → HINTED (Failure)

**Trigger**: Student model fails validation

**Process**:
1. **Increment attempt counter, reset consecutive successes**
   ```python
   question.attempt_count += 1
   question.consecutive_successes = 0  # Reset streak
   ```

2. **Send failure context to frontier model**
   ```python
   hint_request = {
       "question": question.text,
       "student_reasoning": failure_context.thought_tokens,
       "student_code": failure_context.generated_code,
       "error": failure_context.validation_error,
       "existing_hints": question.hints,
       "instruction": "Perform root cause analysis and generate a targeted hint"
   }

   hint = frontier_api.generate_hint(hint_request, model="gpt4")
   ```

   **Example**:
   ```json
   {
       "root_cause": "Student uses integer division incorrectly for Python 3",
       "flawed_assumption": "Assumes `mid = (left + right) / 2` works, but Python 3 returns float",
       "hint": "Remember: Python 3's `/` operator returns float. Use `//` for integer division."
   }
   ```

3. **Append hint to question**
   ```python
   question.hints.append({
       "text": hint.hint,
       "root_cause": hint.root_cause,
       "added_at": timestamp,
       "attempt_number": question.attempt_count
   })

   question.state = "hinted"
   ```

4. **Re-shuffle into dataset**
   ```python
   # Question stays in dataset, will come up again with hints
   dataset.shuffle()  # Ensure it's not immediate next question
   ```

**Net dataset size change**: 0 (question remains)

---

#### Transition 3: HINTED → VARIANT (Success after hints)

**Trigger**: Student succeeds on hinted question

**Process**:
1. **Increment counters**
   ```python
   question.success_count += 1
   question.consecutive_successes += 1
   ```

2. **Create variant WITHOUT hints** (critical)
   ```python
   variant_text = frontier_api.generate_variant(question.text)

   variant_question = {
       "id": f"{question.id}_var_post_hint",
       "text": variant_text,
       "hints": [],  # NO HINTS - test if learning transferred
       "state": "active",
       "parent_question": question.id,
       "was_hinted": True  # Track that parent had hints
   }
   ```

3. **Replace hinted question**
   ```python
   dataset.remove(question.id)
   dataset.add(variant_question)
   ```

**Why remove hints?** Tests if model learned the *concept*, not just memorized hint

---

#### Transition 4: VARIANT → MASTERED

**Trigger**: 3 consecutive successes on variants

**Process**:
1. **Check mastery condition**
   ```python
   if question.consecutive_successes >= 3:
       question.state = "mastered"
   ```

2. **Remove from dataset**
   ```python
   dataset.remove(question.id)
   mastered_archive.store(question)
   ```

3. **Update level stats**
   ```python
   level_stats[question.level].mastered_count += 1
   level_stats[question.level].active_count -= 1
   ```

**Net dataset size change**: -1 (removed, not replaced)

---

## Dataset Structure

### Hierarchical Organization

```python
class CurriculumDataset:
    def __init__(self):
        self.levels = {
            1: LevelDataset(questions=[], level=1),
            2: LevelDataset(questions=[], level=2),
            # ... levels 1-10
        }

        self.global_stats = {
            "total_questions": 0,
            "total_mastered": 0,
            "total_variants_generated": 0,
            "total_hints_added": 0
        }

class LevelDataset:
    def __init__(self, questions, level):
        self.level = level
        self.questions = questions  # List of Question objects
        self.active_pool = []       # Currently trainable
        self.hinted_pool = []       # Failed, have hints
        self.mastered = []          # 3× success, archived

        self.stats = {
            "initial_count": len(questions),
            "current_count": len(questions),
            "mastered_count": 0,
            "variant_count": 0,
            "avg_hints_per_question": 0.0
        }

class Question:
    def __init__(self, text, level, source):
        self.id = generate_id()
        self.text = text
        self.level = level
        self.source = source

        # State tracking
        self.state = "active"
        self.hints = []
        self.variants = []
        self.parent_question = None

        # Counters
        self.attempt_count = 0
        self.success_count = 0
        self.consecutive_successes = 0

        # Metadata
        self.created_at = timestamp()
        self.last_attempted = None
        self.mastered_at = None
```

---

### Indexing for Fast Lookup

**Primary indexes**:
```python
dataset.indexes = {
    "by_id": {question.id: question},
    "by_level": {1: [q1, q2, ...], 2: [...], ...},
    "by_state": {
        "active": [q1, q3, ...],
        "hinted": [q2, q5, ...],
        "mastered": [q4, q6, ...]
    },
    "by_source": {
        "gpt4": [...],
        "claude": [...],
        ...
    }
}
```

**Fast operations**:
```python
# Get all active questions for level 3
active_level3 = dataset.indexes['by_level'][3]
active_only = [q for q in active_level3 if q.state == 'active']

# Or faster:
active_level3 = [q for q in dataset.indexes['by_state']['active'] if q.level == 3]
```

---

## Variant Generation

### Frontier Model Prompts

**Template**:
```
You are helping create training data variants to prevent memorization.

Original question:
"Implement binary search on a sorted array in Python. Return the index of the target element, or -1 if not found."

Task: Create a variant by:
1. Changing nouns (array → list, target → value, etc.)
2. Changing numbers if applicable
3. Keeping the CORE CONCEPT identical (binary search algorithm)
4. Maintaining similar difficulty

Variant question:
```

**Frontier model response**:
```
"Write a binary search function for a sorted list in Python. Return the position of the search value, or -1 if absent."
```

---

### Variant Quality Criteria

**Good variant**:
- ✅ Same core algorithm/concept
- ✅ Different surface wording
- ✅ Similar difficulty
- ✅ Tests understanding, not memorization

**Bad variant**:
- ❌ Changes core concept (binary → linear search)
- ❌ Significantly easier/harder
- ❌ Too similar wording (just swapped 1-2 words)

**Validation**:
```python
def validate_variant(original, variant):
    # 1. Semantic similarity (should be high)
    similarity = compute_semantic_similarity(original.text, variant.text)
    if similarity < 0.7:
        return False, "Concept changed too much"

    # 2. Surface similarity (should be low)
    surface_sim = compute_surface_similarity(original.text, variant.text)
    if surface_sim > 0.8:
        return False, "Too similar wording"

    # 3. Difficulty estimation
    difficulty_delta = estimate_difficulty(variant) - estimate_difficulty(original)
    if abs(difficulty_delta) > 0.5:
        return False, "Difficulty changed significantly"

    return True, "Valid variant"
```

---

### Variant Generation Strategies

**Strategy 1: Noun/Number Substitution** (most common)
```
Original: "Sort an array of 1000 integers using quicksort"
Variant:  "Sort a list of 500 numbers using quicksort"
```

**Strategy 2: Context Change**
```
Original: "Implement a function to find duplicates in a list"
Variant:  "Write code to detect repeated elements in an array"
```

**Strategy 3: Constraint Variation**
```
Original: "Binary search in O(log n) time"
Variant:  "Binary search with at most 20 iterations"
(Same algorithm, different way to express complexity)
```

**Strategy 4: Problem Reframing**
```
Original: "Find the maximum element in a binary tree"
Variant:  "Return the largest value stored in a binary tree"
```

---

## Hint Management

### Hint Generation Process

**Step 1: Root Cause Analysis**

Frontier model receives:
```python
{
    "question": "Implement binary search...",
    "student_reasoning": [
        "I'll use two pointers, left and right",
        "Calculate midpoint as (left + right) / 2",
        "Compare mid element to target",
        ...
    ],
    "student_code": """
def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    while left <= right:
        mid = (left + right) / 2  # BUG: Python 3 float division
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1
""",
    "error": "TypeError: list indices must be integers or slices, not float"
}
```

**Frontier model analysis**:
```json
{
    "root_cause": "Using `/` operator in Python 3, which returns float, not int",
    "flawed_reasoning_step": "Calculate midpoint as (left + right) / 2",
    "correct_reasoning": "Calculate midpoint as (left + right) // 2 (integer division)",
    "hint": "Python 3's `/` operator always returns a float. Use `//` for integer division when calculating array indices."
}
```

---

### Hint Levels

**Level 1 Hint**: Identify the problem area
```
"Check your midpoint calculation - there's an issue with division"
```

**Level 2 Hint**: Explain the concept
```
"Python 3's `/` operator returns float. Array indices must be integers."
```

**Level 3 Hint**: Show the fix
```
"Use `mid = (left + right) // 2` instead of `mid = (left + right) / 2`"
```

**Progression**:
- First failure: Level 1 hint
- Second failure (same question): Add Level 2 hint
- Third failure: Add Level 3 hint
- Fourth+ failure: May indicate concept too hard, skip question

---

### Hint Formatting

**Structured hint**:
```python
{
    "hint_id": "h_1234",
    "question_id": "q_1234",
    "level": 1,  # 1=vague, 2=explanatory, 3=directive
    "text": "Python 3's `/` operator returns float. Use `//` for integer division.",
    "root_cause": "Type error in index calculation",
    "added_at": timestamp,
    "attempt_number": 3,
    "effectiveness": None  # Set after next attempt
}
```

**Display to model**:
```
Question:
"Implement binary search on a sorted array"

Hints:
[1] Check your midpoint calculation - there's an issue with division
[2] Python 3's `/` operator returns float. Array indices must be integers.

Your task:
[Model generates code here]
```

---

### Hint Effectiveness Tracking

**After model attempts hinted question**:

```python
if attempt_succeeded:
    # Hint was effective
    for hint in question.hints:
        hint.effectiveness = "effective"

    # Question transitions to variant (hints removed)

else:
    # Hints not effective yet
    for hint in question.hints:
        if hint.effectiveness is None:
            hint.effectiveness = "ineffective"

    # Add next-level hint
```

**Analytics**:
```python
hint_stats = {
    "total_hints_generated": 5423,
    "effective_hints": 4821,  # 88.9% effectiveness
    "avg_hints_until_success": 1.7,
    "questions_requiring_hints": 2134,  # 42.7% of dataset
}
```

---

## Dataset Shrinkage Mechanics

### Removal Criteria

**Strict mastery definition**: 3 consecutive successes on variants

**Why 3?**
- 1 success: Could be luck
- 2 successes: Building confidence
- 3 successes: Strong evidence of understanding

**Why consecutive?**
- Failure resets counter
- Ensures consistent performance, not sporadic

---

### Shrinkage Dynamics Over Time

**Level 1 example** (2,000 initial questions):

```
Epoch 1:
  Attempts: 2,000 (all questions)
  Successes: 1,200 (60%)
  Failures: 800 (40%)
  → Generate 1,200 variants (replace originals)
  → Add hints to 800 failures
  Dataset size: 2,000 (no change yet)

Epoch 2:
  Attempts: 2,000
  Successes: 1,500 (75% - hints help!)
  Failures: 500
  → 300 questions: 2nd consecutive success (need 1 more)
  → 1,200 questions: 1st consecutive success (need 2 more)
  → 500 questions: failed, add more hints
  Dataset size: 2,000 (no removals yet)

Epoch 3:
  Attempts: 2,000
  Successes: 1,700 (85%)
  Failures: 300
  → 150 questions: 3rd consecutive success → MASTERED
  → Remove 150 questions
  Dataset size: 1,850

Epoch 10:
  Dataset size: 1,200 (40% mastered)

Epoch 20:
  Dataset size: 400 (80% mastered)

Epoch 30:
  Dataset size: 47 (97.6% mastered)

Convergence (Epoch ~40):
  Dataset size: 0-10 (99.5%+ mastered)
```

---

### Mathematical Model

**Dataset size over time**:
```
D(t) = D_0 × (1 - M(t))

where:
  D(t) = dataset size at epoch t
  D_0 = initial size (2,000)
  M(t) = mastery rate at epoch t

Mastery rate model (sigmoid):
  M(t) = 1 / (1 + e^(-k(t - t_0)))

where:
  k = learning rate constant (~0.1)
  t_0 = half-mastery epoch (~15)

Example values:
  M(0) = 0.00 (0% mastered)
  M(10) = 0.18 (18% mastered)
  M(20) = 0.73 (73% mastered)
  M(30) = 0.95 (95% mastered)
  M(40) = 0.99 (99% mastered)

Dataset size:
  D(0) = 2,000
  D(10) = 1,640
  D(20) = 540
  D(30) = 100
  D(40) = 20
```

**Implementation**:
```python
import math

def predict_dataset_size(epoch, D_0=2000, k=0.1, t_0=15):
    mastery_rate = 1 / (1 + math.exp(-k * (epoch - t_0)))
    return int(D_0 * (1 - mastery_rate))

# Predict shrinkage
for epoch in [0, 5, 10, 15, 20, 25, 30]:
    size = predict_dataset_size(epoch)
    print(f"Epoch {epoch}: {size} questions remaining")
```

---

### Convergence Criteria

**When to consider level "complete"?**

**Option 1: Dataset size threshold**
```python
if dataset_size < 50:  # <2.5% of original
    level_complete = True
```

**Option 2: Mastery percentage**
```python
mastery_percent = mastered_count / initial_count
if mastery_percent > 0.95:  # 95%+ mastered
    level_complete = True
```

**Option 3: Epoch limit**
```python
if epoch > 50 and dataset_size < 200:  # Plateau detection
    level_complete = True  # Diminishing returns
```

**Recommended**: Combination
```python
def is_level_complete(epoch, dataset_size, initial_size, mastered_count):
    mastery = mastered_count / initial_size

    # Hard criteria
    if mastery > 0.98:
        return True, "98% mastery achieved"

    if dataset_size < 20:
        return True, "Dataset shrunk to <20 questions"

    # Plateau detection
    if epoch > 40 and mastery > 0.90:
        return True, "Plateau at 90%+ mastery after 40 epochs"

    # Timeout
    if epoch > 80:
        return True, "Epoch limit reached"

    return False, ""
```

---

## Implementation Specifications

### Core Classes

**`CurriculumManager`**:
```python
class CurriculumManager:
    def __init__(self, levels: Dict[int, LevelDataset]):
        self.levels = levels
        self.current_level = 1

        self.frontier_api = FrontierModelAPI()
        self.variant_generator = VariantGenerator(self.frontier_api)
        self.hint_generator = HintGenerator(self.frontier_api)

    def get_next_question(self, level: int) -> Question:
        """Sample next question from level dataset"""
        dataset = self.levels[level]
        return random.choice(dataset.active_pool + dataset.hinted_pool)

    def process_attempt(self, question: Question, result: AttemptResult):
        """Handle success/failure, update question state"""
        if result.success:
            self._handle_success(question)
        else:
            self._handle_failure(question, result)

    def _handle_success(self, question: Question):
        question.success_count += 1
        question.consecutive_successes += 1

        if question.consecutive_successes >= 3:
            # Mastered
            self._remove_question(question)
        else:
            # Generate variant
            variant = self.variant_generator.create_variant(question)
            self._replace_with_variant(question, variant)

    def _handle_failure(self, question: Question, result: AttemptResult):
        question.attempt_count += 1
        question.consecutive_successes = 0

        # Generate hint
        hint = self.hint_generator.generate_hint(
            question, result.reasoning, result.code, result.error
        )

        question.hints.append(hint)
        question.state = "hinted"

    def get_level_stats(self, level: int) -> dict:
        dataset = self.levels[level]
        return {
            "initial_size": dataset.stats["initial_count"],
            "current_size": dataset.stats["current_count"],
            "mastered": dataset.stats["mastered_count"],
            "mastery_percent": dataset.stats["mastered_count"] / dataset.stats["initial_count"],
            "avg_hints": dataset.stats["avg_hints_per_question"]
        }
```

---

### Database Schema (for persistence)

**Questions table**:
```sql
CREATE TABLE questions (
    id VARCHAR PRIMARY KEY,
    text TEXT NOT NULL,
    level INT NOT NULL,
    source VARCHAR NOT NULL,
    state VARCHAR NOT NULL,  -- 'active', 'hinted', 'mastered'
    parent_question_id VARCHAR,
    attempt_count INT DEFAULT 0,
    success_count INT DEFAULT 0,
    consecutive_successes INT DEFAULT 0,
    created_at TIMESTAMP,
    last_attempted TIMESTAMP,
    mastered_at TIMESTAMP
);

CREATE INDEX idx_level_state ON questions(level, state);
```

**Hints table**:
```sql
CREATE TABLE hints (
    id VARCHAR PRIMARY KEY,
    question_id VARCHAR REFERENCES questions(id),
    level INT NOT NULL,  -- 1=vague, 2=explanatory, 3=directive
    text TEXT NOT NULL,
    root_cause TEXT,
    added_at TIMESTAMP,
    attempt_number INT,
    effectiveness VARCHAR  -- 'effective', 'ineffective', NULL
);
```

**Attempts table** (for analytics):
```sql
CREATE TABLE attempts (
    id VARCHAR PRIMARY KEY,
    question_id VARCHAR REFERENCES questions(id),
    success BOOLEAN NOT NULL,
    reasoning_tokens TEXT,
    generated_code TEXT,
    validation_error TEXT,
    attempted_at TIMESTAMP
);
```

---

## Metrics & Monitoring

### Real-Time Metrics

**Per level**:
```python
{
    "level": 3,
    "current_epoch": 12,
    "dataset_size": 1,234,
    "initial_size": 2,000,
    "mastered_count": 766,
    "mastery_percent": 0.383,

    "current_epoch_stats": {
        "attempts": 1,234,
        "successes": 987,
        "failures": 247,
        "success_rate": 0.80
    },

    "hint_stats": {
        "total_hints": 1,543,
        "avg_hints_per_question": 1.25,
        "questions_with_hints": 621
    },

    "variant_stats": {
        "total_variants_generated": 3,421,
        "variant_success_rate": 0.76
    },

    "estimated_epochs_to_completion": 18
}
```

---

### Historical Tracking

**Dataset size over time**:
```python
history = {
    "level": 3,
    "epochs": [
        {"epoch": 1, "size": 2000, "mastered": 0},
        {"epoch": 5, "size": 1847, "mastered": 153},
        {"epoch": 10, "size": 1523, "mastered": 477},
        {"epoch": 15, "size": 982, "mastered": 1018},
        # ...
    ]
}
```

**Visualization**:
```
Dataset Size Over Time (Level 3)
2000 ┤●
1800 ┤ ●●
1600 ┤   ●●
1400 ┤     ●●
1200 ┤       ●●
1000 ┤         ●●●
 800 ┤            ●●●
 600 ┤               ●●●
 400 ┤                  ●●●●
 200 ┤                      ●●●●●
   0 └────────────────────────────●●●
     0   5   10  15  20  25  30  35  40 (epochs)
```

---

### Alerts & Anomalies

**Plateau detection**:
```python
if dataset_size_change < 50 and epoch > 20:
    alert("Plateau detected: Dataset not shrinking. Investigate hint quality.")
```

**Low success rate**:
```python
if success_rate < 0.4:
    alert("Success rate too low. Questions may be too hard for current model.")
```

**Hint explosion**:
```python
if avg_hints_per_question > 3.0:
    alert("Too many hints needed. Consider easier curriculum or better hints.")
```

---

### W&B Integration

**Log metrics**:
```python
import wandb

wandb.log({
    # Dataset dynamics
    f"level_{level}/dataset_size": dataset_size,
    f"level_{level}/mastery_percent": mastery_percent,

    # Performance
    f"level_{level}/success_rate": success_rate,
    f"level_{level}/avg_attempts_per_question": avg_attempts,

    # Hints
    f"level_{level}/total_hints": total_hints,
    f"level_{level}/hint_effectiveness": effective_hints / total_hints,

    # Variants
    f"level_{level}/variants_generated": variant_count,

    # Progress
    f"level_{level}/estimated_completion_epochs": estimate_completion(level)
})
```

---

## Example: Full Level Lifecycle

**Level 1 Training** (start to finish):

```python
# Initialize
curriculum = CurriculumManager(levels={1: level1_dataset})

# Initial state
print(curriculum.get_level_stats(1))
# {'initial_size': 2000, 'current_size': 2000, 'mastered': 0, 'mastery_percent': 0.0}

# Training loop
for epoch in range(50):
    while not curriculum.is_level_complete(1):
        # Get next question
        question = curriculum.get_next_question(level=1)

        # Model attempts
        result = model.attempt_question(question, coding_env)

        # Process result
        curriculum.process_attempt(question, result)

    # Log progress
    stats = curriculum.get_level_stats(1)
    print(f"Epoch {epoch}: {stats['current_size']} questions, {stats['mastery_percent']:.1%} mastered")

    # Check completion
    if stats['mastery_percent'] > 0.95:
        print(f"Level 1 complete! Mastered {stats['mastered']}/{stats['initial_size']} questions")
        break

# Final state
# Epoch 32: 47 questions, 97.6% mastered
# Level 1 complete! Mastered 1953/2000 questions
```

---

**Next Document**: [PHASE5_SELF_MODELING.md](./PHASE5_SELF_MODELING.md)

**Related**:
- [PHASE5_LOGICAL_UNDERSTANDING_V2.md](./PHASE5_LOGICAL_UNDERSTANDING_V2.md)
- [PHASE5_FORMULAS.md](./PHASE5_FORMULAS.md)
- [PHASE5_DATASET_MECHANICS.md](./PHASE5_DATASET_MECHANICS.md)
